import faiss
import numpy as np
import json
from sentence_transformers import SentenceTransformer
from fastapi import FastAPI
from pydantic import BaseModel
import requests  # Для запитів до Ollama API
from fastapi.responses import StreamingResponse
from typing import Callable
import asyncio

# Ініціалізація FastAPI
app = FastAPI()

# Завантаження необхідних компонентів
print("Loading model, FAISS index, and texts...")

# Завантаження SentenceTransformer моделі
model = SentenceTransformer("Data/processed/wiki/sentence_transformer_model")  # Ваша модель векторайзера
print("Model loaded.")

# Завантаження FAISS-індексу
index = faiss.read_index("Data/processed/wiki/vector_index.faiss")
print("FAISS index loaded.")

# Завантаження даних текстів
with open("Data/processed/wiki_processed_results.json", "r", encoding="utf-8") as file:
    texts = json.load(file)
print("Texts loaded.")

# Налаштування API Ollama
OLLAMA_API_URL = "http://127.0.0.1:11434/api/generate"  # Локальний ендпоінт Ollama API


# Допоміжна функція: Знаходження схожих текстів
def find_similar_texts(query: str, k: int = 5):
    # Генерація ембеддингу для запиту
    query_embedding = model.encode([query])

    # Пошук найближчих сусідів у FAISS
    distances, indices = index.search(query_embedding, k)

    # Формування результатів
    results = [{"text": texts[idx], "distance": float(distance)} for idx, distance in zip(indices[0], distances[0])]
    return results


# Допоміжна функція: Запит до Ollama
def query_ollama(model_name: str, prompt: str):
    # Структура даних для запиту
    data = {
        "model": model_name,  # Назва моделі в Ollama
        "prompt": prompt,
    }

    # Робимо запит до Ollama API
    with requests.post(OLLAMA_API_URL, json=data, stream=True) as response:
        if response.status_code == 200:
            try:
                # Ініціалізуємо змінну для збору частин відповіді
                full_response = ""

                # Перебираємо частини стрімінгової відповіді
                for chunk in response.iter_lines(decode_unicode=True):
                    if chunk:  # Перевіряємо, щоб частина не була порожньою
                        try:
                            # Декодуємо частину JSON
                            chunk_data = json.loads(chunk)

                            # Перевіряємо, чи є ключ "response"
                            if "response" in chunk_data:
                                full_response += chunk_data["response"]
                        except json.JSONDecodeError:
                            # Ігноруємо некоректні фрагменти
                            continue

                # Якщо після збору відповіді вона порожня, повертаємо помилку
                if not full_response.strip():
                    return "No response generated by the model. Check the prompt or context for relevancy."

                return full_response.strip()  # Повертаємо зібрану відповідь
            except Exception as e:
                # Якщо сталася будь-яка інша помилка
                raise Exception(f"Failed to process streaming response from Ollama API: {e}")
        else:
            # Якщо статус відповіді не 200, повідомляємо про помилку
            raise Exception(f"Ollama API error: {response.status_code}, {response.text}")



# FastAPI маршрут: Запит до агента
class QueryRequest(BaseModel):
    query: str
    num_results: int = 5  # Кількість результатів FAISS

async def generate_response_stream(model_name: str, prompt: str):
    # Структура запиту до Ollama
    data = {"model": model_name, "prompt": prompt}

    # Відправляємо запит зі стрімінгом
    with requests.post(OLLAMA_API_URL, json=data, stream=True) as response:
        if response.status_code == 200:
            # Читаємо окремі частини відповіді
            for chunk in response.iter_lines(decode_unicode=True):
                if chunk:
                    try:
                        chunk_data = json.loads(chunk)
                        if "response" in chunk_data:
                            # Повертаємо наступну частину відповіді
                            yield f"{chunk_data['response']}"
                    except json.JSONDecodeError:
                        # Ігноруємо некоректний JSON
                        continue
        else:
            # Якщо сталася помилка, завершуємо стрімінг та повертаємо повідомлення
            yield f"Error: {response.status_code} {response.text}"
@app.post("/agent")
async def agent_endpoint_stream(request: QueryRequest):
    try:
        # Крок 1: Пошук схожих текстів у FAISS
        similar_texts = find_similar_texts(request.query, request.num_results)

        if not similar_texts:
            # Якщо немає релевантного контексту
            return StreamingResponse(iter([{
                "response": "Немає релевантного контексту до вашого запиту. Спробуйте уточнити або змінити запит."
            }]), media_type="application/json")

        # Формуємо контекст
        context = "\n\n".join([
            f"Context {i + 1}: {result['text']['processed_text'][:500]}"  # Обрізання для уникнення надлишку тексту
            for i, result in enumerate(similar_texts)
        ])

        # Формуємо промпт для Ollama
        prompt = f"Використовуй контекст щоб відповісти на запит:\n\n{context}\n\nЗапит: {request.query}\nВідповідь:"

        # Генеруємо стрімінг відповіді Ollama
        response_stream: Callable = generate_response_stream("phi4", prompt)

        # Повертаємо стрімінгову відповідь
        return StreamingResponse(response_stream, media_type="text/plain")

    except Exception as e:
        # Відправляємо повідомлення про помилку у стрімінговому вигляді
        return StreamingResponse(iter([f"Error: {str(e)}"]), media_type="text/plain")